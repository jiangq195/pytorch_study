{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7178, 0.0943, 0.9869],\n",
       "        [0.5790, 0.9639, 0.7871],\n",
       "        [0.9866, 0.0744, 0.0626],\n",
       "        [0.7637, 0.6454, 0.9932],\n",
       "        [0.6864, 0.1867, 0.0661]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.type>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5.5,3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5,3,dtype=torch.double)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0973, 0.9636, 0.0220],\n",
       "        [0.0774, 0.5849, 0.5398],\n",
       "        [0.7931, 0.1740, 0.4892],\n",
       "        [0.6696, 0.1470, 0.8457],\n",
       "        [0.1220, 0.0871, 0.4244]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand_like(x,dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9706, 0.0975, 0.8280],\n",
       "        [0.1070, 0.8733, 0.5320],\n",
       "        [0.3528, 0.6038, 0.3970],\n",
       "        [0.1006, 0.9064, 0.0180],\n",
       "        [0.8224, 0.4002, 0.3973]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0679, 1.0611, 0.8500],\n",
       "        [0.1844, 1.4582, 1.0718],\n",
       "        [1.1459, 0.7778, 0.8862],\n",
       "        [0.7703, 1.0533, 0.8637],\n",
       "        [0.9444, 0.4873, 0.8217]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x +y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0679, 1.0611, 0.8500],\n",
       "        [0.1844, 1.4582, 1.0718],\n",
       "        [1.1459, 0.7778, 0.8862],\n",
       "        [0.7703, 1.0533, 0.8637],\n",
       "        [0.9444, 0.4873, 0.8217]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0679, 1.0611, 0.8500],\n",
       "        [0.1844, 1.4582, 1.0718],\n",
       "        [1.1459, 0.7778, 0.8862],\n",
       "        [0.7703, 1.0533, 0.8637],\n",
       "        [0.9444, 0.4873, 0.8217]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.empty(5,3)\n",
    "torch.add(x,y,out=result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0679, 1.0611, 0.8500],\n",
       "        [0.1844, 1.4582, 1.0718],\n",
       "        [1.1459, 0.7778, 0.8862],\n",
       "        [0.7703, 1.0533, 0.8637],\n",
       "        [0.9444, 0.4873, 0.8217]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0973, 0.9636, 0.0220],\n",
       "        [0.0774, 0.5849, 0.5398],\n",
       "        [0.7931, 0.1740, 0.4892],\n",
       "        [0.6696, 0.1470, 0.8457],\n",
       "        [0.1220, 0.0871, 0.4244]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0973, 0.9636, 0.0220],\n",
       "        [0.0774, 0.5849, 0.5398],\n",
       "        [0.7931, 0.1740, 0.4892],\n",
       "        [0.6696, 0.1470, 0.8457],\n",
       "        [0.1220, 0.0871, 0.4244]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9636, 0.0220],\n",
       "        [0.5849, 0.5398],\n",
       "        [0.1740, 0.4892],\n",
       "        [0.1470, 0.8457],\n",
       "        [0.0871, 0.4244]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2648, -1.7337,  0.3081,  0.2916,  1.0064, -0.5937,  0.6856,  0.7685],\n",
       "        [-1.1100,  0.0162,  1.3621, -0.7600,  1.1492,  2.6333, -0.8367, -0.1553]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y = x.reshape(2,8)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7932])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7932])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7932124137878418"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2648, -1.7337,  0.3081,  0.2916,  1.0064, -0.5937,  0.6856,  0.7685],\n",
       "        [-1.1100,  0.0162,  1.3621, -0.7600,  1.1492,  2.6333, -0.8367, -0.1553]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2648, -1.1100],\n",
       "        [-1.7337,  0.0162],\n",
       "        [ 0.3081,  1.3621],\n",
       "        [ 0.2916, -0.7600],\n",
       "        [ 1.0064,  1.1492],\n",
       "        [-0.5937,  2.6333],\n",
       "        [ 0.6856, -0.8367],\n",
       "        [ 0.7685, -0.1553]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(a,1,out=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a +1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.add(a,1,out=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nvidia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-bc6386d60349>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnvidia\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msmi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nvidia' is not defined"
     ]
    }
   ],
   "source": [
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_visible_devices=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7932])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-795404c7d5aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library."
     ]
    }
   ],
   "source": [
    "x.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37947000.0\n",
      "1 39459980.0\n",
      "2 43855320.0\n",
      "3 41846364.0\n",
      "4 30120704.0\n",
      "5 16027488.0\n",
      "6 7162514.0\n",
      "7 3359399.0\n",
      "8 1927278.0\n",
      "9 1338941.0\n",
      "10 1040842.625\n",
      "11 853140.375\n",
      "12 716818.6875\n",
      "13 610181.0\n",
      "14 523906.53125\n",
      "15 452785.84375\n",
      "16 393460.28125\n",
      "17 343549.53125\n",
      "18 301317.6875\n",
      "19 265395.09375\n",
      "20 234634.140625\n",
      "21 208130.765625\n",
      "22 185210.421875\n",
      "23 165314.671875\n",
      "24 147961.9375\n",
      "25 132768.125\n",
      "26 119414.859375\n",
      "27 107657.359375\n",
      "28 97269.71875\n",
      "29 88063.328125\n",
      "30 79886.640625\n",
      "31 72556.46875\n",
      "32 66013.1875\n",
      "33 60160.71875\n",
      "34 54914.14453125\n",
      "35 50201.25\n",
      "36 45964.3125\n",
      "37 42148.04296875\n",
      "38 38703.7265625\n",
      "39 35586.03125\n",
      "40 32761.578125\n",
      "41 30199.30859375\n",
      "42 27878.61328125\n",
      "43 25765.458984375\n",
      "44 23837.40234375\n",
      "45 22077.23828125\n",
      "46 20467.46484375\n",
      "47 18994.373046875\n",
      "48 17643.646484375\n",
      "49 16404.30859375\n",
      "50 15264.6982421875\n",
      "51 14216.4306640625\n",
      "52 13251.1318359375\n",
      "53 12361.26953125\n",
      "54 11541.4375\n",
      "55 10784.2685546875\n",
      "56 10084.4765625\n",
      "57 9436.626953125\n",
      "58 8836.1015625\n",
      "59 8279.548828125\n",
      "60 7763.08740234375\n",
      "61 7283.44482421875\n",
      "62 6837.71484375\n",
      "63 6422.85986328125\n",
      "64 6036.60498046875\n",
      "65 5676.9189453125\n",
      "66 5341.3994140625\n",
      "67 5028.39892578125\n",
      "68 4736.18310546875\n",
      "69 4463.0546875\n",
      "70 4207.68017578125\n",
      "71 3968.7529296875\n",
      "72 3745.091064453125\n",
      "73 3535.616943359375\n",
      "74 3339.31884765625\n",
      "75 3155.170654296875\n",
      "76 2982.366455078125\n",
      "77 2820.15185546875\n",
      "78 2667.783203125\n",
      "79 2524.541748046875\n",
      "80 2389.844482421875\n",
      "81 2263.281982421875\n",
      "82 2144.043701171875\n",
      "83 2031.7080078125\n",
      "84 1925.8861083984375\n",
      "85 1826.1793212890625\n",
      "86 1732.181396484375\n",
      "87 1643.481689453125\n",
      "88 1559.793212890625\n",
      "89 1480.7259521484375\n",
      "90 1406.0595703125\n",
      "91 1335.5228271484375\n",
      "92 1268.8818359375\n",
      "93 1205.8702392578125\n",
      "94 1146.287841796875\n",
      "95 1089.9039306640625\n",
      "96 1036.5469970703125\n",
      "97 986.0236206054688\n",
      "98 938.1865234375\n",
      "99 892.8660888671875\n",
      "100 849.9154663085938\n",
      "101 809.2000732421875\n",
      "102 770.5880126953125\n",
      "103 733.962646484375\n",
      "104 699.2236938476562\n",
      "105 666.2630615234375\n",
      "106 634.9675903320312\n",
      "107 605.24658203125\n",
      "108 577.01904296875\n",
      "109 550.2080078125\n",
      "110 524.736328125\n",
      "111 500.52276611328125\n",
      "112 477.5032958984375\n",
      "113 455.61187744140625\n",
      "114 434.798828125\n",
      "115 415.002197265625\n",
      "116 396.160400390625\n",
      "117 378.2286376953125\n",
      "118 361.1632995605469\n",
      "119 344.9154968261719\n",
      "120 329.44366455078125\n",
      "121 314.7056884765625\n",
      "122 300.6664123535156\n",
      "123 287.2956848144531\n",
      "124 274.5558776855469\n",
      "125 262.40960693359375\n",
      "126 250.8311309814453\n",
      "127 239.79225158691406\n",
      "128 229.2684783935547\n",
      "129 219.228759765625\n",
      "130 209.6545867919922\n",
      "131 200.52317810058594\n",
      "132 191.80885314941406\n",
      "133 183.4930877685547\n",
      "134 175.55569458007812\n",
      "135 167.97914123535156\n",
      "136 160.7469940185547\n",
      "137 153.84112548828125\n",
      "138 147.24697875976562\n",
      "139 140.94737243652344\n",
      "140 134.9319305419922\n",
      "141 129.1859588623047\n",
      "142 123.6949691772461\n",
      "143 118.44872283935547\n",
      "144 113.43521881103516\n",
      "145 108.64461517333984\n",
      "146 104.06620025634766\n",
      "147 99.6864013671875\n",
      "148 95.49873352050781\n",
      "149 91.49755096435547\n",
      "150 87.66914367675781\n",
      "151 84.00865936279297\n",
      "152 80.50619506835938\n",
      "153 77.15829467773438\n",
      "154 73.95439147949219\n",
      "155 70.88899993896484\n",
      "156 67.95535278320312\n",
      "157 65.14820861816406\n",
      "158 62.460731506347656\n",
      "159 59.89007568359375\n",
      "160 57.428466796875\n",
      "161 55.072303771972656\n",
      "162 52.81593322753906\n",
      "163 50.65665054321289\n",
      "164 48.5876350402832\n",
      "165 46.607505798339844\n",
      "166 44.70991134643555\n",
      "167 42.893402099609375\n",
      "168 41.1528434753418\n",
      "169 39.485328674316406\n",
      "170 37.88832092285156\n",
      "171 36.358116149902344\n",
      "172 34.89227294921875\n",
      "173 33.48600387573242\n",
      "174 32.13922119140625\n",
      "175 30.849693298339844\n",
      "176 29.612905502319336\n",
      "177 28.426849365234375\n",
      "178 27.29038429260254\n",
      "179 26.200450897216797\n",
      "180 25.155912399291992\n",
      "181 24.155527114868164\n",
      "182 23.196250915527344\n",
      "183 22.27711296081543\n",
      "184 21.39438247680664\n",
      "185 20.54838752746582\n",
      "186 19.736473083496094\n",
      "187 18.958335876464844\n",
      "188 18.211593627929688\n",
      "189 17.495149612426758\n",
      "190 16.80694580078125\n",
      "191 16.147626876831055\n",
      "192 15.514974594116211\n",
      "193 14.907346725463867\n",
      "194 14.324224472045898\n",
      "195 13.764972686767578\n",
      "196 13.227798461914062\n",
      "197 12.71251392364502\n",
      "198 12.217223167419434\n",
      "199 11.741744995117188\n",
      "200 11.28585433959961\n",
      "201 10.847648620605469\n",
      "202 10.426873207092285\n",
      "203 10.023261070251465\n",
      "204 9.635437965393066\n",
      "205 9.26307487487793\n",
      "206 8.905646324157715\n",
      "207 8.562383651733398\n",
      "208 8.232697486877441\n",
      "209 7.915973663330078\n",
      "210 7.611382007598877\n",
      "211 7.319129943847656\n",
      "212 7.03854513168335\n",
      "213 6.768946647644043\n",
      "214 6.509893417358398\n",
      "215 6.260899066925049\n",
      "216 6.021604061126709\n",
      "217 5.792184829711914\n",
      "218 5.571443557739258\n",
      "219 5.359303951263428\n",
      "220 5.155160903930664\n",
      "221 4.9592108726501465\n",
      "222 4.770848274230957\n",
      "223 4.5898051261901855\n",
      "224 4.4157891273498535\n",
      "225 4.24860143661499\n",
      "226 4.088077545166016\n",
      "227 3.933465003967285\n",
      "228 3.7847845554351807\n",
      "229 3.6421427726745605\n",
      "230 3.5046463012695312\n",
      "231 3.3725979328155518\n",
      "232 3.245816230773926\n",
      "233 3.123767375946045\n",
      "234 3.006312131881714\n",
      "235 2.893439292907715\n",
      "236 2.7849392890930176\n",
      "237 2.680663824081421\n",
      "238 2.5803446769714355\n",
      "239 2.4838099479675293\n",
      "240 2.3908426761627197\n",
      "241 2.301603317260742\n",
      "242 2.2158658504486084\n",
      "243 2.133009433746338\n",
      "244 2.0536274909973145\n",
      "245 1.9771798849105835\n",
      "246 1.903564691543579\n",
      "247 1.8328934907913208\n",
      "248 1.7647812366485596\n",
      "249 1.6991891860961914\n",
      "250 1.6362481117248535\n",
      "251 1.5756076574325562\n",
      "252 1.517346739768982\n",
      "253 1.461111068725586\n",
      "254 1.407137393951416\n",
      "255 1.355114221572876\n",
      "256 1.3050816059112549\n",
      "257 1.25698721408844\n",
      "258 1.2106815576553345\n",
      "259 1.1660200357437134\n",
      "260 1.1230460405349731\n",
      "261 1.0817666053771973\n",
      "262 1.0419509410858154\n",
      "263 1.0037522315979004\n",
      "264 0.9669095277786255\n",
      "265 0.9313717484474182\n",
      "266 0.897232174873352\n",
      "267 0.8643029928207397\n",
      "268 0.8326619863510132\n",
      "269 0.8022416830062866\n",
      "270 0.7728942036628723\n",
      "271 0.7446790337562561\n",
      "272 0.7174134254455566\n",
      "273 0.691326379776001\n",
      "274 0.6661150455474854\n",
      "275 0.6417705416679382\n",
      "276 0.6183706521987915\n",
      "277 0.5958784222602844\n",
      "278 0.5741723775863647\n",
      "279 0.5533273816108704\n",
      "280 0.5332393646240234\n",
      "281 0.513793408870697\n",
      "282 0.4951387047767639\n",
      "283 0.4772198796272278\n",
      "284 0.45995593070983887\n",
      "285 0.4432579278945923\n",
      "286 0.42718011140823364\n",
      "287 0.4117357134819031\n",
      "288 0.3968770503997803\n",
      "289 0.3825049102306366\n",
      "290 0.36868301033973694\n",
      "291 0.35541701316833496\n",
      "292 0.34248366951942444\n",
      "293 0.3301789164543152\n",
      "294 0.31831562519073486\n",
      "295 0.306851863861084\n",
      "296 0.2957664430141449\n",
      "297 0.28513285517692566\n",
      "298 0.2749349772930145\n",
      "299 0.26500651240348816\n",
      "300 0.25551459193229675\n",
      "301 0.24632029235363007\n",
      "302 0.23747286200523376\n",
      "303 0.22896800935268402\n",
      "304 0.22081860899925232\n",
      "305 0.21286115050315857\n",
      "306 0.20519769191741943\n",
      "307 0.19783681631088257\n",
      "308 0.190788134932518\n",
      "309 0.1839451938867569\n",
      "310 0.17737071216106415\n",
      "311 0.17102330923080444\n",
      "312 0.16492238640785217\n",
      "313 0.1590418815612793\n",
      "314 0.15336324274539948\n",
      "315 0.14789755642414093\n",
      "316 0.14262115955352783\n",
      "317 0.13753706216812134\n",
      "318 0.1326335072517395\n",
      "319 0.12794706225395203\n",
      "320 0.12338817864656448\n",
      "321 0.11899363994598389\n",
      "322 0.11478421092033386\n",
      "323 0.1106872707605362\n",
      "324 0.10676310956478119\n",
      "325 0.10298258811235428\n",
      "326 0.09932763129472733\n",
      "327 0.0958133339881897\n",
      "328 0.0924193412065506\n",
      "329 0.08914566040039062\n",
      "330 0.08596405386924744\n",
      "331 0.08294941484928131\n",
      "332 0.08000455051660538\n",
      "333 0.07719586044549942\n",
      "334 0.074464350938797\n",
      "335 0.07183323800563812\n",
      "336 0.06930316984653473\n",
      "337 0.06683123111724854\n",
      "338 0.06448611617088318\n",
      "339 0.06221673637628555\n",
      "340 0.06002982705831528\n",
      "341 0.057918161153793335\n",
      "342 0.05586721748113632\n",
      "343 0.053907521069049835\n",
      "344 0.0520152822136879\n",
      "345 0.050176773220300674\n",
      "346 0.04843631014227867\n",
      "347 0.04671803489327431\n",
      "348 0.045081332325935364\n",
      "349 0.04349455609917641\n",
      "350 0.04198912903666496\n",
      "351 0.04052675515413284\n",
      "352 0.03912656009197235\n",
      "353 0.03774947300553322\n",
      "354 0.036430954933166504\n",
      "355 0.03514735400676727\n",
      "356 0.0339345820248127\n",
      "357 0.03274982422590256\n",
      "358 0.03162172809243202\n",
      "359 0.030501876026391983\n",
      "360 0.029444275423884392\n",
      "361 0.028419703245162964\n",
      "362 0.02743799425661564\n",
      "363 0.02649020217359066\n",
      "364 0.02557387761771679\n",
      "365 0.024686414748430252\n",
      "366 0.023823242634534836\n",
      "367 0.02299998328089714\n",
      "368 0.02221757546067238\n",
      "369 0.021445007994771004\n",
      "370 0.020706424489617348\n",
      "371 0.019982878118753433\n",
      "372 0.019301922991871834\n",
      "373 0.0186386127024889\n",
      "374 0.017996899783611298\n",
      "375 0.017383864149451256\n",
      "376 0.0167907252907753\n",
      "377 0.016217084601521492\n",
      "378 0.015657030045986176\n",
      "379 0.015118520706892014\n",
      "380 0.014606437645852566\n",
      "381 0.014102783985435963\n",
      "382 0.013632725924253464\n",
      "383 0.013166964054107666\n",
      "384 0.012713592499494553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 0.012287657707929611\n",
      "386 0.011870144866406918\n",
      "387 0.011467319913208485\n",
      "388 0.01108665019273758\n",
      "389 0.010715019889175892\n",
      "390 0.010353736579418182\n",
      "391 0.010012085549533367\n",
      "392 0.009686363860964775\n",
      "393 0.009359030984342098\n",
      "394 0.009043112397193909\n",
      "395 0.00874789897352457\n",
      "396 0.008454421535134315\n",
      "397 0.008179723285138607\n",
      "398 0.007911289110779762\n",
      "399 0.007649085018783808\n",
      "400 0.007392858155071735\n",
      "401 0.007153566926717758\n",
      "402 0.006926096044480801\n",
      "403 0.0067006382159888744\n",
      "404 0.006475954316556454\n",
      "405 0.006272772327065468\n",
      "406 0.006064959801733494\n",
      "407 0.005869001615792513\n",
      "408 0.005683434661477804\n",
      "409 0.0055016446858644485\n",
      "410 0.005322807468473911\n",
      "411 0.005155643448233604\n",
      "412 0.004987324122339487\n",
      "413 0.004830278921872377\n",
      "414 0.004676001146435738\n",
      "415 0.004529751371592283\n",
      "416 0.004382905084639788\n",
      "417 0.004247037693858147\n",
      "418 0.00411306694149971\n",
      "419 0.003983974922448397\n",
      "420 0.0038630510680377483\n",
      "421 0.0037427651695907116\n",
      "422 0.003628444392234087\n",
      "423 0.003518868237733841\n",
      "424 0.003408035496249795\n",
      "425 0.0033032456412911415\n",
      "426 0.003202109830453992\n",
      "427 0.0031058478634804487\n",
      "428 0.003010851563885808\n",
      "429 0.002920328639447689\n",
      "430 0.002833150327205658\n",
      "431 0.0027480802964419127\n",
      "432 0.0026652072556316853\n",
      "433 0.0025850594975054264\n",
      "434 0.0025072270072996616\n",
      "435 0.002434659516438842\n",
      "436 0.002363909035921097\n",
      "437 0.002296261955052614\n",
      "438 0.0022288018371909857\n",
      "439 0.002162476535886526\n",
      "440 0.0021013328805565834\n",
      "441 0.0020399224013090134\n",
      "442 0.001982885878533125\n",
      "443 0.0019258613465353847\n",
      "444 0.0018718704814091325\n",
      "445 0.0018192457500845194\n",
      "446 0.0017683933256193995\n",
      "447 0.0017200371949002147\n",
      "448 0.0016713020158931613\n",
      "449 0.0016251355409622192\n",
      "450 0.0015805988805368543\n",
      "451 0.0015377576928585768\n",
      "452 0.0014980629784986377\n",
      "453 0.001456332509405911\n",
      "454 0.0014177454868331552\n",
      "455 0.0013782652094960213\n",
      "456 0.0013414790155366063\n",
      "457 0.0013076616451144218\n",
      "458 0.0012746298452839255\n",
      "459 0.0012398955877870321\n",
      "460 0.0012077236315235496\n",
      "461 0.0011769174598157406\n",
      "462 0.0011462783440947533\n",
      "463 0.0011165455216541886\n",
      "464 0.0010894652223214507\n",
      "465 0.001062100171111524\n",
      "466 0.0010368437506258488\n",
      "467 0.0010105390101671219\n",
      "468 0.0009858604753389955\n",
      "469 0.000960049859713763\n",
      "470 0.000936271739192307\n",
      "471 0.0009120777831412852\n",
      "472 0.0008908737800084054\n",
      "473 0.0008683197665959597\n",
      "474 0.0008497536764480174\n",
      "475 0.0008290668483823538\n",
      "476 0.0008091601775959134\n",
      "477 0.0007889870903454721\n",
      "478 0.0007716096006333828\n",
      "479 0.0007543687243014574\n",
      "480 0.0007378761656582355\n",
      "481 0.000721036281902343\n",
      "482 0.0007046738173812628\n",
      "483 0.0006872654194012284\n",
      "484 0.0006739070522598922\n",
      "485 0.0006572160637006164\n",
      "486 0.0006426638574339449\n",
      "487 0.000627808622084558\n",
      "488 0.0006149272085167468\n",
      "489 0.0006012652302160859\n",
      "490 0.0005891774781048298\n",
      "491 0.0005752359284088016\n",
      "492 0.0005634074914269149\n",
      "493 0.0005503476713784039\n",
      "494 0.000538521446287632\n",
      "495 0.0005279668257571757\n",
      "496 0.0005167849594727159\n",
      "497 0.0005060408147983253\n",
      "498 0.0004945594118908048\n",
      "499 0.0004852197889704257\n"
     ]
    }
   ],
   "source": [
    "N, D_in,H,D_out = 64,1000,100,10\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "\n",
    "w1 = torch.randn(D_in,H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    h = x.mm(w1) # N*H\n",
    "    h_relu = h.clamp(min=0) # N*H\n",
    "    y_pred = h_relu.mm(w2) # N*D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = np.square(y_pred-y).sum().item()\n",
    "    print(it,loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 =h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # update weight of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5., grad_fn=<AddBackward0>)\n",
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "y = w*x+b\n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29166854.0\n",
      "1 31334746.0\n",
      "2 43075964.0\n",
      "3 57439448.0\n",
      "4 59300824.0\n",
      "5 38722752.0\n",
      "6 15093683.0\n",
      "7 4540087.5\n",
      "8 1828968.5\n",
      "9 1150358.375\n",
      "10 895820.8125\n",
      "11 743962.125\n",
      "12 630860.625\n",
      "13 540238.5625\n",
      "14 465913.25\n",
      "15 404314.3125\n",
      "16 352799.03125\n",
      "17 309353.0\n",
      "18 272415.65625\n",
      "19 240814.71875\n",
      "20 213628.171875\n",
      "21 190080.734375\n",
      "22 169607.921875\n",
      "23 151750.5\n",
      "24 136114.875\n",
      "25 122384.3203125\n",
      "26 110285.8125\n",
      "27 99579.265625\n",
      "28 90087.5546875\n",
      "29 81643.015625\n",
      "30 74109.4453125\n",
      "31 67388.5546875\n",
      "32 61373.38671875\n",
      "33 55971.71875\n",
      "34 51109.74609375\n",
      "35 46727.05078125\n",
      "36 42769.921875\n",
      "37 39196.8828125\n",
      "38 35968.13671875\n",
      "39 33038.578125\n",
      "40 30374.19140625\n",
      "41 27949.84375\n",
      "42 25738.814453125\n",
      "43 23720.771484375\n",
      "44 21879.12890625\n",
      "45 20195.69140625\n",
      "46 18655.650390625\n",
      "47 17242.552734375\n",
      "48 15946.53125\n",
      "49 14757.1650390625\n",
      "50 13664.7421875\n",
      "51 12660.51953125\n",
      "52 11736.31640625\n",
      "53 10885.4677734375\n",
      "54 10101.7275390625\n",
      "55 9379.083984375\n",
      "56 8712.607421875\n",
      "57 8097.76513671875\n",
      "58 7530.0458984375\n",
      "59 7005.251953125\n",
      "60 6519.88720703125\n",
      "61 6070.74609375\n",
      "62 5654.892578125\n",
      "63 5269.6669921875\n",
      "64 4912.6865234375\n",
      "65 4582.49365234375\n",
      "66 4276.25732421875\n",
      "67 3991.904541015625\n",
      "68 3727.746826171875\n",
      "69 3482.330322265625\n",
      "70 3254.255859375\n",
      "71 3042.077880859375\n",
      "72 2844.71533203125\n",
      "73 2661.012451171875\n",
      "74 2490.039794921875\n",
      "75 2331.0830078125\n",
      "76 2183.29541015625\n",
      "77 2045.5257568359375\n",
      "78 1917.0333251953125\n",
      "79 1797.15576171875\n",
      "80 1685.25830078125\n",
      "81 1580.775146484375\n",
      "82 1483.2288818359375\n",
      "83 1392.1240234375\n",
      "84 1306.92529296875\n",
      "85 1227.290771484375\n",
      "86 1152.829345703125\n",
      "87 1083.1519775390625\n",
      "88 1017.9332885742188\n",
      "89 956.875244140625\n",
      "90 899.7119140625\n",
      "91 846.1746826171875\n",
      "92 796.0297241210938\n",
      "93 749.03076171875\n",
      "94 704.96728515625\n",
      "95 663.6468505859375\n",
      "96 624.8828735351562\n",
      "97 588.51611328125\n",
      "98 554.3861083984375\n",
      "99 522.3468627929688\n",
      "100 492.27630615234375\n",
      "101 464.0472717285156\n",
      "102 437.509521484375\n",
      "103 412.5723876953125\n",
      "104 389.1387939453125\n",
      "105 367.106201171875\n",
      "106 346.3936767578125\n",
      "107 326.9098815917969\n",
      "108 308.5862121582031\n",
      "109 291.3406982421875\n",
      "110 275.109375\n",
      "111 259.8410949707031\n",
      "112 245.4536895751953\n",
      "113 231.90643310546875\n",
      "114 219.14561462402344\n",
      "115 207.12423706054688\n",
      "116 195.79507446289062\n",
      "117 185.11624145507812\n",
      "118 175.0472412109375\n",
      "119 165.55496215820312\n",
      "120 156.60028076171875\n",
      "121 148.1556396484375\n",
      "122 140.19259643554688\n",
      "123 132.67276000976562\n",
      "124 125.5766372680664\n",
      "125 118.87801361083984\n",
      "126 112.55372619628906\n",
      "127 106.5805892944336\n",
      "128 100.93816375732422\n",
      "129 95.61151123046875\n",
      "130 90.57542419433594\n",
      "131 85.81893157958984\n",
      "132 81.32427215576172\n",
      "133 77.07586669921875\n",
      "134 73.0571060180664\n",
      "135 69.25712585449219\n",
      "136 65.66485595703125\n",
      "137 62.26496124267578\n",
      "138 59.0483512878418\n",
      "139 56.00587463378906\n",
      "140 53.12541198730469\n",
      "141 50.400169372558594\n",
      "142 47.82052230834961\n",
      "143 45.38002395629883\n",
      "144 43.06768035888672\n",
      "145 40.87628173828125\n",
      "146 38.80255126953125\n",
      "147 36.83698272705078\n",
      "148 34.975467681884766\n",
      "149 33.21142578125\n",
      "150 31.539535522460938\n",
      "151 29.95570182800293\n",
      "152 28.453689575195312\n",
      "153 27.0296688079834\n",
      "154 25.67962074279785\n",
      "155 24.401386260986328\n",
      "156 23.18741798400879\n",
      "157 22.035947799682617\n",
      "158 20.942928314208984\n",
      "159 19.90691375732422\n",
      "160 18.923627853393555\n",
      "161 17.990182876586914\n",
      "162 17.105148315429688\n",
      "163 16.264493942260742\n",
      "164 15.466498374938965\n",
      "165 14.709217071533203\n",
      "166 13.990217208862305\n",
      "167 13.307663917541504\n",
      "168 12.659065246582031\n",
      "169 12.042752265930176\n",
      "170 11.457786560058594\n",
      "171 10.902237892150879\n",
      "172 10.374344825744629\n",
      "173 9.872673034667969\n",
      "174 9.396095275878906\n",
      "175 8.943222999572754\n",
      "176 8.51286506652832\n",
      "177 8.103683471679688\n",
      "178 7.714855670928955\n",
      "179 7.345109939575195\n",
      "180 6.993433475494385\n",
      "181 6.659094333648682\n",
      "182 6.341090202331543\n",
      "183 6.039041519165039\n",
      "184 5.751494407653809\n",
      "185 5.477685451507568\n",
      "186 5.2177019119262695\n",
      "187 4.970189571380615\n",
      "188 4.734982490539551\n",
      "189 4.511122703552246\n",
      "190 4.297863960266113\n",
      "191 4.095236301422119\n",
      "192 3.9023520946502686\n",
      "193 3.718376398086548\n",
      "194 3.5439021587371826\n",
      "195 3.377385377883911\n",
      "196 3.218977212905884\n",
      "197 3.0680809020996094\n",
      "198 2.924499988555908\n",
      "199 2.7876672744750977\n",
      "200 2.6574089527130127\n",
      "201 2.5334808826446533\n",
      "202 2.4154748916625977\n",
      "203 2.3027970790863037\n",
      "204 2.195971965789795\n",
      "205 2.0938045978546143\n",
      "206 1.9968366622924805\n",
      "207 1.9041874408721924\n",
      "208 1.8159652948379517\n",
      "209 1.731802225112915\n",
      "210 1.65178382396698\n",
      "211 1.5755470991134644\n",
      "212 1.5029933452606201\n",
      "213 1.4336702823638916\n",
      "214 1.3675647974014282\n",
      "215 1.3047144412994385\n",
      "216 1.244691014289856\n",
      "217 1.1875135898590088\n",
      "218 1.1329731941223145\n",
      "219 1.0810638666152954\n",
      "220 1.0315929651260376\n",
      "221 0.9842551946640015\n",
      "222 0.9392718076705933\n",
      "223 0.8963274955749512\n",
      "224 0.8554468750953674\n",
      "225 0.8163033127784729\n",
      "226 0.7791681289672852\n",
      "227 0.7436365485191345\n",
      "228 0.7097680568695068\n",
      "229 0.6775543689727783\n",
      "230 0.6467323303222656\n",
      "231 0.6173889636993408\n",
      "232 0.5893701910972595\n",
      "233 0.5626983642578125\n",
      "234 0.537142276763916\n",
      "235 0.5128800272941589\n",
      "236 0.48960432410240173\n",
      "237 0.46747922897338867\n",
      "238 0.4463030695915222\n",
      "239 0.4262259900569916\n",
      "240 0.40699440240859985\n",
      "241 0.38867950439453125\n",
      "242 0.3711049556732178\n",
      "243 0.35438627004623413\n",
      "244 0.3384280502796173\n",
      "245 0.3231765031814575\n",
      "246 0.3086540400981903\n",
      "247 0.29482707381248474\n",
      "248 0.28158846497535706\n",
      "249 0.2689293324947357\n",
      "250 0.25684401392936707\n",
      "251 0.24536311626434326\n",
      "252 0.23434807360172272\n",
      "253 0.22387240827083588\n",
      "254 0.2138022482395172\n",
      "255 0.20425432920455933\n",
      "256 0.19516050815582275\n",
      "257 0.18639186024665833\n",
      "258 0.1780892312526703\n",
      "259 0.17018458247184753\n",
      "260 0.16257771849632263\n",
      "261 0.1553770899772644\n",
      "262 0.14844274520874023\n",
      "263 0.14183686673641205\n",
      "264 0.13552893698215485\n",
      "265 0.129484161734581\n",
      "266 0.12370476871728897\n",
      "267 0.11821098625659943\n",
      "268 0.11298327147960663\n",
      "269 0.1079770028591156\n",
      "270 0.1032077968120575\n",
      "271 0.09863653033971786\n",
      "272 0.0942855253815651\n",
      "273 0.0900968536734581\n",
      "274 0.08610056340694427\n",
      "275 0.08230311423540115\n",
      "276 0.07864439487457275\n",
      "277 0.07514867186546326\n",
      "278 0.0718383714556694\n",
      "279 0.06867710500955582\n",
      "280 0.0656379759311676\n",
      "281 0.06273148953914642\n",
      "282 0.05996498093008995\n",
      "283 0.05731945484876633\n",
      "284 0.054810792207717896\n",
      "285 0.0523957759141922\n",
      "286 0.05009300261735916\n",
      "287 0.04788277670741081\n",
      "288 0.04577399790287018\n",
      "289 0.04376314580440521\n",
      "290 0.04186037927865982\n",
      "291 0.04001070559024811\n",
      "292 0.03825858607888222\n",
      "293 0.03658388927578926\n",
      "294 0.03497747331857681\n",
      "295 0.033438313752412796\n",
      "296 0.031979043036699295\n",
      "297 0.030573884025216103\n",
      "298 0.02924126200377941\n",
      "299 0.027962908148765564\n",
      "300 0.026746727526187897\n",
      "301 0.025597166270017624\n",
      "302 0.02447567880153656\n",
      "303 0.023413056507706642\n",
      "304 0.0223831944167614\n",
      "305 0.021410569548606873\n",
      "306 0.020481985062360764\n",
      "307 0.0196040328592062\n",
      "308 0.018756909295916557\n",
      "309 0.017938019707798958\n",
      "310 0.017165109515190125\n",
      "311 0.016428234055638313\n",
      "312 0.01573018915951252\n",
      "313 0.015047646127641201\n",
      "314 0.014398735016584396\n",
      "315 0.013793598860502243\n",
      "316 0.013200826942920685\n",
      "317 0.012629671022295952\n",
      "318 0.012098381295800209\n",
      "319 0.011579710990190506\n",
      "320 0.01108713448047638\n",
      "321 0.010612905956804752\n",
      "322 0.010163147933781147\n",
      "323 0.009737569838762283\n",
      "324 0.009327147156000137\n",
      "325 0.00893428549170494\n",
      "326 0.008554383181035519\n",
      "327 0.008198247291147709\n",
      "328 0.007859854027628899\n",
      "329 0.007531654089689255\n",
      "330 0.007220112252980471\n",
      "331 0.006919737905263901\n",
      "332 0.006634538061916828\n",
      "333 0.006365024019032717\n",
      "334 0.006100018043071032\n",
      "335 0.005853606387972832\n",
      "336 0.005611982196569443\n",
      "337 0.005380979739129543\n",
      "338 0.005166449584066868\n",
      "339 0.0049581388011574745\n",
      "340 0.00475687300786376\n",
      "341 0.00456690089777112\n",
      "342 0.00437878118827939\n",
      "343 0.004203789867460728\n",
      "344 0.004034969490021467\n",
      "345 0.0038756374269723892\n",
      "346 0.0037236404605209827\n",
      "347 0.0035728898365050554\n",
      "348 0.0034339178819209337\n",
      "349 0.003304282668977976\n",
      "350 0.0031739729456603527\n",
      "351 0.0030514956451952457\n",
      "352 0.002934888703748584\n",
      "353 0.0028228049632161856\n",
      "354 0.002717300783842802\n",
      "355 0.002610099036246538\n",
      "356 0.00250889640301466\n",
      "357 0.002413834910839796\n",
      "358 0.002322646789252758\n",
      "359 0.0022374687250703573\n",
      "360 0.0021540450397878885\n",
      "361 0.0020733673591166735\n",
      "362 0.001995740458369255\n",
      "363 0.0019222963601350784\n",
      "364 0.0018527524080127478\n",
      "365 0.0017862631939351559\n",
      "366 0.001723932451568544\n",
      "367 0.001662006019614637\n",
      "368 0.001603769836947322\n",
      "369 0.001548976986669004\n",
      "370 0.001495238277129829\n",
      "371 0.0014419006183743477\n",
      "372 0.0013921300414949656\n",
      "373 0.0013441001065075397\n",
      "374 0.0012990400427952409\n",
      "375 0.0012544299243018031\n",
      "376 0.001213039387948811\n",
      "377 0.0011727885575965047\n",
      "378 0.0011331505374982953\n",
      "379 0.0010950235882773995\n",
      "380 0.0010585124837234616\n",
      "381 0.0010233898647129536\n",
      "382 0.000991161447018385\n",
      "383 0.0009595883311703801\n",
      "384 0.0009283369872719049\n",
      "385 0.0008987769251689315\n",
      "386 0.0008709424873813987\n",
      "387 0.0008464195998385549\n",
      "388 0.0008179998840205371\n",
      "389 0.0007932409644126892\n",
      "390 0.0007696487009525299\n",
      "391 0.0007466627866961062\n",
      "392 0.0007237658719532192\n",
      "393 0.0007020006887614727\n",
      "394 0.0006820665439590812\n",
      "395 0.0006622511427849531\n",
      "396 0.0006424201419577003\n",
      "397 0.0006234843167476356\n",
      "398 0.0006063237087801099\n",
      "399 0.0005888130981475115\n",
      "400 0.0005717750755138695\n",
      "401 0.0005555276875384152\n",
      "402 0.0005404379917308688\n",
      "403 0.0005256528966128826\n",
      "404 0.0005115217063575983\n",
      "405 0.0004987888969480991\n",
      "406 0.0004842210910283029\n",
      "407 0.0004711038200184703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408 0.00045905340812169015\n",
      "409 0.00044712406815961003\n",
      "410 0.0004352443211246282\n",
      "411 0.0004239724366925657\n",
      "412 0.0004136207571718842\n",
      "413 0.00040260283276438713\n",
      "414 0.00039207839290611446\n",
      "415 0.00038237296394072473\n",
      "416 0.0003731157921720296\n",
      "417 0.00036280302447266877\n",
      "418 0.00035420365747995675\n",
      "419 0.0003452099044807255\n",
      "420 0.0003367123717907816\n",
      "421 0.000328605470713228\n",
      "422 0.00032010546419769526\n",
      "423 0.0003118645690847188\n",
      "424 0.0003042615717276931\n",
      "425 0.00029670263756997883\n",
      "426 0.0002901267434936017\n",
      "427 0.0002828676369972527\n",
      "428 0.0002764873206615448\n",
      "429 0.00027041338034905493\n",
      "430 0.0002641350438352674\n",
      "431 0.00025881524197757244\n",
      "432 0.00025170124717988074\n",
      "433 0.00024643796496093273\n",
      "434 0.000241316098254174\n",
      "435 0.0002359942882321775\n",
      "436 0.00023113260976970196\n",
      "437 0.00022618456569034606\n",
      "438 0.00022125142277218401\n",
      "439 0.0002162035380024463\n",
      "440 0.00021220400230959058\n",
      "441 0.00020712046534754336\n",
      "442 0.00020287311053834856\n",
      "443 0.00019928766414523125\n",
      "444 0.00019486223754938692\n",
      "445 0.00019136571791023016\n",
      "446 0.0001873568689916283\n",
      "447 0.00018357632507104427\n",
      "448 0.00018002954311668873\n",
      "449 0.00017674898845143616\n",
      "450 0.00017307829693891108\n",
      "451 0.0001691256184130907\n",
      "452 0.00016596654313616455\n",
      "453 0.00016284474986605346\n",
      "454 0.00015963014448061585\n",
      "455 0.00015667361731175333\n",
      "456 0.00015426063328050077\n",
      "457 0.00015115960559342057\n",
      "458 0.00014802950317971408\n",
      "459 0.00014558904513251036\n",
      "460 0.0001426547096343711\n",
      "461 0.000139670490170829\n",
      "462 0.0001372563128825277\n",
      "463 0.0001347069046460092\n",
      "464 0.00013243017019703984\n",
      "465 0.0001303030294366181\n",
      "466 0.0001283062738366425\n",
      "467 0.00012579666508827358\n",
      "468 0.00012365804286673665\n",
      "469 0.0001218380784848705\n",
      "470 0.00011942152195842937\n",
      "471 0.00011718427413143218\n",
      "472 0.00011540761624928564\n",
      "473 0.00011371634900569916\n",
      "474 0.00011175378313055262\n",
      "475 0.00011025718413293362\n",
      "476 0.00010841520997928455\n",
      "477 0.000106712723209057\n",
      "478 0.00010471101268194616\n",
      "479 0.00010301876318408176\n",
      "480 0.00010162118269363418\n",
      "481 0.00010007688251789659\n",
      "482 9.821620187722147e-05\n",
      "483 9.69134271144867e-05\n",
      "484 9.536700963508338e-05\n",
      "485 9.371292981086299e-05\n",
      "486 9.232464071828872e-05\n",
      "487 9.109316306421533e-05\n",
      "488 8.965114830061793e-05\n",
      "489 8.832687308313325e-05\n",
      "490 8.714575960766524e-05\n",
      "491 8.618793071946129e-05\n",
      "492 8.484694262733683e-05\n",
      "493 8.330558193847537e-05\n",
      "494 8.207435166696087e-05\n",
      "495 8.0992525909096e-05\n",
      "496 8.00302077550441e-05\n",
      "497 7.921348151285201e-05\n",
      "498 7.790577365085483e-05\n",
      "499 7.664188160561025e-05\n"
     ]
    }
   ],
   "source": [
    "N, D_in,H,D_out = 64,1000,100,10\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "\n",
    "w1 = torch.randn(D_in,H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "#     h = x.mm(w1) # N*H\n",
    "#     h_relu = h.clamp(min=0) # N*H\n",
    "#     y_pred = h_relu.mm(w2) # N*D_out\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred-y).pow(2).sum()  # computation graph\n",
    "    print(it,loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    \n",
    "    # update weight of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 43536352.0\n"
     ]
    }
   ],
   "source": [
    "N, D_in,H,D_out = 64,1000,100,10\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "\n",
    "w1 = torch.randn(D_in,H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "\n",
    "# Forward pass\n",
    "#     h = x.mm(w1) # N*H\n",
    "#     h_relu = h.clamp(min=0) # N*H\n",
    "#     y_pred = h_relu.mm(w2) # N*D_out\n",
    "y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "# compute loss\n",
    "loss = (y_pred-y).pow(2).sum()  # computation graph\n",
    "print(it,loss.item())\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31635860.0\n",
      "1 27107598.0\n",
      "2 26624052.0\n",
      "3 25516990.0\n",
      "4 21934362.0\n",
      "5 15997405.0\n",
      "6 10196501.0\n",
      "7 5922331.5\n",
      "8 3411336.25\n",
      "9 2061038.125\n",
      "10 1355400.75\n",
      "11 970904.375\n",
      "12 745685.5625\n",
      "13 600942.0\n",
      "14 499663.6875\n",
      "15 423677.4375\n",
      "16 363781.75\n",
      "17 315017.3125\n",
      "18 274496.78125\n",
      "19 240400.90625\n",
      "20 211410.53125\n",
      "21 186581.78125\n",
      "22 165234.6875\n",
      "23 146772.015625\n",
      "24 130730.34375\n",
      "25 116727.71875\n",
      "26 104454.34375\n",
      "27 93672.390625\n",
      "28 84176.8984375\n",
      "29 75792.5078125\n",
      "30 68363.8125\n",
      "31 61766.6484375\n",
      "32 55900.8984375\n",
      "33 50669.14453125\n",
      "34 45993.62109375\n",
      "35 41809.1875\n",
      "36 38056.09765625\n",
      "37 34683.96484375\n",
      "38 31647.845703125\n",
      "39 28911.521484375\n",
      "40 26441.962890625\n",
      "41 24208.453125\n",
      "42 22187.03125\n",
      "43 20354.859375\n",
      "44 18691.955078125\n",
      "45 17181.169921875\n",
      "46 15807.70703125\n",
      "47 14556.7890625\n",
      "48 13416.8837890625\n",
      "49 12376.8798828125\n",
      "50 11427.0224609375\n",
      "51 10557.6943359375\n",
      "52 9761.72265625\n",
      "53 9032.5869140625\n",
      "54 8363.5927734375\n",
      "55 7749.2177734375\n",
      "56 7184.732421875\n",
      "57 6665.8671875\n",
      "58 6188.60107421875\n",
      "59 5748.99609375\n",
      "60 5343.78564453125\n",
      "61 4969.97021484375\n",
      "62 4624.88623046875\n",
      "63 4305.9736328125\n",
      "64 4011.25341796875\n",
      "65 3738.672607421875\n",
      "66 3486.253173828125\n",
      "67 3252.4072265625\n",
      "68 3035.781982421875\n",
      "69 2834.822021484375\n",
      "70 2648.4501953125\n",
      "71 2475.480224609375\n",
      "72 2315.037841796875\n",
      "73 2165.880859375\n",
      "74 2027.2174072265625\n",
      "75 1898.1624755859375\n",
      "76 1778.01123046875\n",
      "77 1666.09326171875\n",
      "78 1561.7830810546875\n",
      "79 1464.517333984375\n",
      "80 1373.75439453125\n",
      "81 1289.0592041015625\n",
      "82 1210.0013427734375\n",
      "83 1136.1500244140625\n",
      "84 1067.12158203125\n",
      "85 1002.5924682617188\n",
      "86 942.2520751953125\n",
      "87 885.8016357421875\n",
      "88 832.991943359375\n",
      "89 783.5439453125\n",
      "90 737.2286987304688\n",
      "91 693.8381958007812\n",
      "92 653.1642456054688\n",
      "93 615.0313720703125\n",
      "94 579.2640380859375\n",
      "95 545.7057495117188\n",
      "96 514.2171020507812\n",
      "97 484.65679931640625\n",
      "98 456.8973693847656\n",
      "99 430.8365783691406\n",
      "100 406.3432312011719\n",
      "101 383.3196716308594\n",
      "102 361.68292236328125\n",
      "103 341.3406677246094\n",
      "104 322.2021789550781\n",
      "105 304.19775390625\n",
      "106 287.2535705566406\n",
      "107 271.3067321777344\n",
      "108 256.2894287109375\n",
      "109 242.14869689941406\n",
      "110 228.82855224609375\n",
      "111 216.28103637695312\n",
      "112 204.45262145996094\n",
      "113 193.30593872070312\n",
      "114 182.79751586914062\n",
      "115 172.88636779785156\n",
      "116 163.5419158935547\n",
      "117 154.72654724121094\n",
      "118 146.40841674804688\n",
      "119 138.5585479736328\n",
      "120 131.1481475830078\n",
      "121 124.15154266357422\n",
      "122 117.54816436767578\n",
      "123 111.31034851074219\n",
      "124 105.41551208496094\n",
      "125 99.85022735595703\n",
      "126 94.59123229980469\n",
      "127 89.62226104736328\n",
      "128 84.92252349853516\n",
      "129 80.48300170898438\n",
      "130 76.28279876708984\n",
      "131 72.3105697631836\n",
      "132 68.55509185791016\n",
      "133 65.00270080566406\n",
      "134 61.64065170288086\n",
      "135 58.45960235595703\n",
      "136 55.450382232666016\n",
      "137 52.60020446777344\n",
      "138 49.90256118774414\n",
      "139 47.349281311035156\n",
      "140 44.93149185180664\n",
      "141 42.64155197143555\n",
      "142 40.4732666015625\n",
      "143 38.41825485229492\n",
      "144 36.47235107421875\n",
      "145 34.627655029296875\n",
      "146 32.88050079345703\n",
      "147 31.22429084777832\n",
      "148 29.655664443969727\n",
      "149 28.1675968170166\n",
      "150 26.758102416992188\n",
      "151 25.42043113708496\n",
      "152 24.152021408081055\n",
      "153 22.94890594482422\n",
      "154 21.80813980102539\n",
      "155 20.725706100463867\n",
      "156 19.698503494262695\n",
      "157 18.724079132080078\n",
      "158 17.799259185791016\n",
      "159 16.92209815979004\n",
      "160 16.08942222595215\n",
      "161 15.299330711364746\n",
      "162 14.549330711364746\n",
      "163 13.836103439331055\n",
      "164 13.16004753112793\n",
      "165 12.517255783081055\n",
      "166 11.907463073730469\n",
      "167 11.328160285949707\n",
      "168 10.77767276763916\n",
      "169 10.254559516906738\n",
      "170 9.757779121398926\n",
      "171 9.285778999328613\n",
      "172 8.837580680847168\n",
      "173 8.41178035736084\n",
      "174 8.006247520446777\n",
      "175 7.6216583251953125\n",
      "176 7.255749702453613\n",
      "177 6.907835960388184\n",
      "178 6.577119827270508\n",
      "179 6.262759208679199\n",
      "180 5.963757514953613\n",
      "181 5.679977893829346\n",
      "182 5.409508228302002\n",
      "183 5.152172565460205\n",
      "184 4.907341003417969\n",
      "185 4.675044536590576\n",
      "186 4.453521728515625\n",
      "187 4.243163585662842\n",
      "188 4.042572021484375\n",
      "189 3.852092981338501\n",
      "190 3.6705875396728516\n",
      "191 3.4980123043060303\n",
      "192 3.3337323665618896\n",
      "193 3.1775591373443604\n",
      "194 3.028597593307495\n",
      "195 2.8869338035583496\n",
      "196 2.7519969940185547\n",
      "197 2.623706340789795\n",
      "198 2.5013062953948975\n",
      "199 2.384939670562744\n",
      "200 2.2739768028259277\n",
      "201 2.1683566570281982\n",
      "202 2.0679562091827393\n",
      "203 1.9721325635910034\n",
      "204 1.8807828426361084\n",
      "205 1.794002890586853\n",
      "206 1.7111178636550903\n",
      "207 1.632190227508545\n",
      "208 1.5568711757659912\n",
      "209 1.4853366613388062\n",
      "210 1.417077660560608\n",
      "211 1.3520597219467163\n",
      "212 1.2902202606201172\n",
      "213 1.2310870885849\n",
      "214 1.174786925315857\n",
      "215 1.1211787462234497\n",
      "216 1.0699676275253296\n",
      "217 1.0212645530700684\n",
      "218 0.9746986031532288\n",
      "219 0.9304687976837158\n",
      "220 0.888128936290741\n",
      "221 0.847784698009491\n",
      "222 0.8093829154968262\n",
      "223 0.7726964950561523\n",
      "224 0.737753689289093\n",
      "225 0.704346239566803\n",
      "226 0.672612190246582\n",
      "227 0.6422929763793945\n",
      "228 0.613376796245575\n",
      "229 0.5857232809066772\n",
      "230 0.5594655871391296\n",
      "231 0.5343727469444275\n",
      "232 0.5104020833969116\n",
      "233 0.48750126361846924\n",
      "234 0.4656384289264679\n",
      "235 0.4448821544647217\n",
      "236 0.42493635416030884\n",
      "237 0.4059910476207733\n",
      "238 0.3878946602344513\n",
      "239 0.3706541061401367\n",
      "240 0.3540717661380768\n",
      "241 0.33837151527404785\n",
      "242 0.32330065965652466\n",
      "243 0.30895835161209106\n",
      "244 0.29526084661483765\n",
      "245 0.2822433114051819\n",
      "246 0.2697208821773529\n",
      "247 0.2578158676624298\n",
      "248 0.24640673398971558\n",
      "249 0.23556329309940338\n",
      "250 0.22519828379154205\n",
      "251 0.21526823937892914\n",
      "252 0.20580410957336426\n",
      "253 0.19675180315971375\n",
      "254 0.1880924105644226\n",
      "255 0.179861381649971\n",
      "256 0.17198406159877777\n",
      "257 0.16439203917980194\n",
      "258 0.15722355246543884\n",
      "259 0.1503686010837555\n",
      "260 0.14381758868694305\n",
      "261 0.13751842081546783\n",
      "262 0.13150259852409363\n",
      "263 0.1257992684841156\n",
      "264 0.12029596418142319\n",
      "265 0.11509142816066742\n",
      "266 0.11007875204086304\n",
      "267 0.10528110712766647\n",
      "268 0.10070144385099411\n",
      "269 0.09635955840349197\n",
      "270 0.09217748790979385\n",
      "271 0.08819752186536789\n",
      "272 0.08437798917293549\n",
      "273 0.08074624836444855\n",
      "274 0.07727134972810745\n",
      "275 0.07393305748701096\n",
      "276 0.07076115906238556\n",
      "277 0.0676918551325798\n",
      "278 0.06478185951709747\n",
      "279 0.06200645491480827\n",
      "280 0.059340644627809525\n",
      "281 0.0568012036383152\n",
      "282 0.05435226857662201\n",
      "283 0.05204008147120476\n",
      "284 0.049824148416519165\n",
      "285 0.04768197610974312\n",
      "286 0.045649293810129166\n",
      "287 0.04370315372943878\n",
      "288 0.04184892028570175\n",
      "289 0.0400589182972908\n",
      "290 0.03836886212229729\n",
      "291 0.03674735128879547\n",
      "292 0.035194192081689835\n",
      "293 0.033700212836265564\n",
      "294 0.03227246552705765\n",
      "295 0.030904335901141167\n",
      "296 0.029614105820655823\n",
      "297 0.02837190218269825\n",
      "298 0.02715742401778698\n",
      "299 0.02601514756679535\n",
      "300 0.02493082731962204\n",
      "301 0.023882940411567688\n",
      "302 0.022879669442772865\n",
      "303 0.02193201705813408\n",
      "304 0.021014366298913956\n",
      "305 0.020139142870903015\n",
      "306 0.019306836649775505\n",
      "307 0.01850907690823078\n",
      "308 0.01773213781416416\n",
      "309 0.016998399049043655\n",
      "310 0.01628950983285904\n",
      "311 0.015625767409801483\n",
      "312 0.014982882887125015\n",
      "313 0.014354858547449112\n",
      "314 0.01376406941562891\n",
      "315 0.013205064460635185\n",
      "316 0.012660869397222996\n",
      "317 0.012141766957938671\n",
      "318 0.011640133336186409\n",
      "319 0.011172874830663204\n",
      "320 0.010713852941989899\n",
      "321 0.010284487158060074\n",
      "322 0.009863397106528282\n",
      "323 0.009464581497013569\n",
      "324 0.009083252400159836\n",
      "325 0.008717694319784641\n",
      "326 0.00836782157421112\n",
      "327 0.008031469769775867\n",
      "328 0.007713395170867443\n",
      "329 0.007404412142932415\n",
      "330 0.007113548927009106\n",
      "331 0.0068307192996144295\n",
      "332 0.0065566180273890495\n",
      "333 0.006294745486229658\n",
      "334 0.006052130833268166\n",
      "335 0.005812060087919235\n",
      "336 0.005588950123637915\n",
      "337 0.005367118399590254\n",
      "338 0.005161100998520851\n",
      "339 0.00496230972930789\n",
      "340 0.004769846796989441\n",
      "341 0.004586860537528992\n",
      "342 0.004409326706081629\n",
      "343 0.004243729170411825\n",
      "344 0.00407822709530592\n",
      "345 0.0039276424795389175\n",
      "346 0.0037780506536364555\n",
      "347 0.0036358381621539593\n",
      "348 0.003502610605210066\n",
      "349 0.0033717157784849405\n",
      "350 0.0032469499856233597\n",
      "351 0.003127141622826457\n",
      "352 0.0030101751908659935\n",
      "353 0.0028994421008974314\n",
      "354 0.002795506501570344\n",
      "355 0.0026930051390081644\n",
      "356 0.0025954865850508213\n",
      "357 0.0025054842699319124\n",
      "358 0.002414342248812318\n",
      "359 0.00232892669737339\n",
      "360 0.0022479710169136524\n",
      "361 0.0021694947499781847\n",
      "362 0.002093792427331209\n",
      "363 0.0020185839384794235\n",
      "364 0.0019491311395540833\n",
      "365 0.0018819913966581225\n",
      "366 0.0018178817117586732\n",
      "367 0.0017565294401720166\n",
      "368 0.001696579740382731\n",
      "369 0.0016388003714382648\n",
      "370 0.001584613579325378\n",
      "371 0.0015322426334023476\n",
      "372 0.0014823618112131953\n",
      "373 0.001433059573173523\n",
      "374 0.0013857820304110646\n",
      "375 0.0013403637567535043\n",
      "376 0.001298669259995222\n",
      "377 0.0012564336648210883\n",
      "378 0.001216784818097949\n",
      "379 0.001177808502689004\n",
      "380 0.001142338034696877\n",
      "381 0.001106727751903236\n",
      "382 0.0010730982758104801\n",
      "383 0.0010402820771560073\n",
      "384 0.001007321523502469\n",
      "385 0.0009790330659598112\n",
      "386 0.0009506094502285123\n",
      "387 0.0009189199772663414\n",
      "388 0.0008919304236769676\n",
      "389 0.0008663093904033303\n",
      "390 0.0008398932404816151\n",
      "391 0.0008161342120729387\n",
      "392 0.0007933845627121627\n",
      "393 0.0007702250732108951\n",
      "394 0.0007483285735361278\n",
      "395 0.0007266424363479018\n",
      "396 0.0007053151493892074\n",
      "397 0.0006870499346405268\n",
      "398 0.0006685658008791506\n",
      "399 0.0006482754251919687\n",
      "400 0.0006302812253125012\n",
      "401 0.0006138755124993622\n",
      "402 0.0005959777045063674\n",
      "403 0.0005791972507722676\n",
      "404 0.000563662382774055\n",
      "405 0.0005500887054949999\n",
      "406 0.0005361053626984358\n",
      "407 0.000521517067681998\n",
      "408 0.0005084082949906588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409 0.0004954725154675543\n",
      "410 0.00048245140351355076\n",
      "411 0.00046945252688601613\n",
      "412 0.0004586777067743242\n",
      "413 0.0004463146033231169\n",
      "414 0.00043410007492639124\n",
      "415 0.00042413637856952846\n",
      "416 0.00041336860158480704\n",
      "417 0.000402922451030463\n",
      "418 0.0003931753453798592\n",
      "419 0.00038291336386464536\n",
      "420 0.00037356687244027853\n",
      "421 0.00036517129046842456\n",
      "422 0.00035697437124326825\n",
      "423 0.00034852378303185105\n",
      "424 0.0003398479602765292\n",
      "425 0.00033279391936957836\n",
      "426 0.0003253202303312719\n",
      "427 0.0003171775315422565\n",
      "428 0.00030938367126509547\n",
      "429 0.0003025399928446859\n",
      "430 0.0002965499006677419\n",
      "431 0.0002906280860770494\n",
      "432 0.0002833210164681077\n",
      "433 0.0002767696569208056\n",
      "434 0.00027114301337860525\n",
      "435 0.0002657551085576415\n",
      "436 0.00025988442939706147\n",
      "437 0.0002543682639952749\n",
      "438 0.00024891982320696115\n",
      "439 0.00024326902348548174\n",
      "440 0.00023853172024246305\n",
      "441 0.00023425246763508767\n",
      "442 0.00022850178356748074\n",
      "443 0.00022395812266040593\n",
      "444 0.0002191413368564099\n",
      "445 0.00021473973174579442\n",
      "446 0.0002103533479385078\n",
      "447 0.00020600623975042254\n",
      "448 0.00020127324387431145\n",
      "449 0.00019699342374224216\n",
      "450 0.00019347359193488955\n",
      "451 0.00018967541109304875\n",
      "452 0.0001856922754086554\n",
      "453 0.00018263522360939533\n",
      "454 0.0001787224318832159\n",
      "455 0.00017561324057169259\n",
      "456 0.0001725795737002045\n",
      "457 0.00016955762112047523\n",
      "458 0.00016641443653497845\n",
      "459 0.00016330397920683026\n",
      "460 0.00015989552775863558\n",
      "461 0.00015679377247579396\n",
      "462 0.000154138047946617\n",
      "463 0.00015141666517592967\n",
      "464 0.00014866524725221097\n",
      "465 0.00014604210446123034\n",
      "466 0.0001433816214557737\n",
      "467 0.00014126846508588642\n",
      "468 0.00013906801177654415\n",
      "469 0.0001366085052723065\n",
      "470 0.00013396469876170158\n",
      "471 0.0001318607246503234\n",
      "472 0.0001295203110203147\n",
      "473 0.00012741699174512178\n",
      "474 0.00012522224278654903\n",
      "475 0.00012322035036049783\n",
      "476 0.00012082975445082411\n",
      "477 0.00011927806190215051\n",
      "478 0.00011730236292351037\n",
      "479 0.00011511460616020486\n",
      "480 0.00011300476035103202\n",
      "481 0.00011152680235682055\n",
      "482 0.0001100302833947353\n",
      "483 0.00010804063640534878\n",
      "484 0.00010674563236534595\n",
      "485 0.00010510938591323793\n",
      "486 0.00010358467989135534\n",
      "487 0.00010205410944763571\n",
      "488 0.00010060298518510535\n",
      "489 9.9281343864277e-05\n",
      "490 9.752665209816769e-05\n",
      "491 9.602621867088601e-05\n",
      "492 9.417843102710322e-05\n",
      "493 9.282740211347118e-05\n",
      "494 9.164231596514583e-05\n",
      "495 9.01069724932313e-05\n",
      "496 8.896174404071644e-05\n",
      "497 8.76756603247486e-05\n",
      "498 8.64681278471835e-05\n",
      "499 8.53619712870568e-05\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in,H,D_out = 64,1000,100,10\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "\n",
    "# w1 = torch.randn(D_in,H, requires_grad=True)\n",
    "# w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "# learning_rate = 1e-4\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred , y)  # computation graph\n",
    "    print(it,loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    # update weight of w1 and w2\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.8084, -0.6575, -0.3067,  ...,  1.7378,  1.9232, -0.3874],\n",
       "        [ 0.8342, -0.9789,  0.5655,  ..., -0.2236, -0.1784,  0.9570],\n",
       "        [-1.1518, -1.3509, -2.8544,  ..., -1.1010,  0.8274,  1.9028],\n",
       "        ...,\n",
       "        [ 0.8895, -1.1872, -1.3862,  ...,  1.6319,  0.2891, -0.0971],\n",
       "        [-2.0606,  1.9076, -0.2625,  ..., -1.3040, -0.5768, -0.6194],\n",
       "        [ 2.4754,  0.0963, -0.0172,  ...,  1.0128,  0.3246,  1.3429]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CPU but got backend CUDA for argument #2 'mat2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-2e029c8e52d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# compute loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-2e029c8e52d3>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1407\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1408\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1409\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1410\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of backend CPU but got backend CUDA for argument #2 'mat2'"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in,H,D_out = 64,1000,100,10\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N,D_out)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear2(self.linear1(x).clamp(min=0))\n",
    "        return y_pred\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(D_in, H, bias=False),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(H, D_out, bias=False),\n",
    "# )\n",
    "\n",
    "# torch.nn.init.normal_(model[0].weight)\n",
    "# torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred , y)  # computation graph\n",
    "    print(it,loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    # update weight of w1 and w2\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
